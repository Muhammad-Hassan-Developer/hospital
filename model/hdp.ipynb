{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec305af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:57.254927Z",
     "iopub.status.busy": "2024-12-07T05:39:57.254439Z",
     "iopub.status.idle": "2024-12-07T05:39:58.163440Z",
     "shell.execute_reply": "2024-12-07T05:39:58.162522Z"
    },
    "id": "TBIaOOcWJcpA",
    "papermill": {
     "duration": 0.920355,
     "end_time": "2024-12-07T05:39:58.166053",
     "exception": false,
     "start_time": "2024-12-07T05:39:57.245698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd33f46",
   "metadata": {
    "id": "ih_reofRMj7V",
    "papermill": {
     "duration": 0.005293,
     "end_time": "2024-12-07T05:39:58.177268",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.171975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4126a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.190171Z",
     "iopub.status.busy": "2024-12-07T05:39:58.189643Z",
     "iopub.status.idle": "2024-12-07T05:39:58.214519Z",
     "shell.execute_reply": "2024-12-07T05:39:58.213469Z"
    },
    "id": "eT427Z1sJ3bB",
    "papermill": {
     "duration": 0.034084,
     "end_time": "2024-12-07T05:39:58.216951",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.182867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6846be85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.231003Z",
     "iopub.status.busy": "2024-12-07T05:39:58.230670Z",
     "iopub.status.idle": "2024-12-07T05:39:58.256315Z",
     "shell.execute_reply": "2024-12-07T05:39:58.255129Z"
    },
    "id": "ZxpphfNBKCrV",
    "outputId": "02c396c3-7188-4e51-fa8d-f488723ab1cf",
    "papermill": {
     "duration": 0.034881,
     "end_time": "2024-12-07T05:39:58.258700",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.223819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67912a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.272223Z",
     "iopub.status.busy": "2024-12-07T05:39:58.271819Z",
     "iopub.status.idle": "2024-12-07T05:39:58.278069Z",
     "shell.execute_reply": "2024-12-07T05:39:58.276799Z"
    },
    "id": "SLcny9SvJvIb",
    "outputId": "466f9c1d-d7c2-4a7f-82ed-ce2fc143fd02",
    "papermill": {
     "duration": 0.01577,
     "end_time": "2024-12-07T05:39:58.280515",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.264745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79358b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.293984Z",
     "iopub.status.busy": "2024-12-07T05:39:58.293654Z",
     "iopub.status.idle": "2024-12-07T05:39:58.300855Z",
     "shell.execute_reply": "2024-12-07T05:39:58.299749Z"
    },
    "id": "Wf4x5ltrk5Uc",
    "outputId": "c0640b81-e93b-4799-c427-f9bda6cf36ef",
    "papermill": {
     "duration": 0.016554,
     "end_time": "2024-12-07T05:39:58.303101",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.286547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_index = 5  # Replace 5 with the desired row number (starting from 0)\n",
    "specific_row = data.iloc[row_index]\n",
    "print(specific_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d5ffe",
   "metadata": {
    "id": "zRUfJP21K4Zg",
    "papermill": {
     "duration": 0.005703,
     "end_time": "2024-12-07T05:39:58.314924",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.309221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac35e1",
   "metadata": {
    "id": "UvfEYtD3Lf1u",
    "papermill": {
     "duration": 0.005553,
     "end_time": "2024-12-07T05:39:58.326475",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.320922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.1 Data Exploration\n",
    "The dataset was thoroughly examined to ensure its integrity and readiness for analysis. Key steps included checking for missing values, verifying data types, and generating summary statistics. The dataset was found to be complete, with no missing entries, which facilitated a straightforward preprocessing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020cf1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.340678Z",
     "iopub.status.busy": "2024-12-07T05:39:58.340327Z",
     "iopub.status.idle": "2024-12-07T05:39:58.350999Z",
     "shell.execute_reply": "2024-12-07T05:39:58.349866Z"
    },
    "id": "pJek6lVYKxWs",
    "outputId": "fcf04402-52c1-47c3-e4e4-dc6add0becf2",
    "papermill": {
     "duration": 0.020795,
     "end_time": "2024-12-07T05:39:58.353275",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.332480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb1367",
   "metadata": {
    "id": "XS0wHEssLNBp",
    "papermill": {
     "duration": 0.00596,
     "end_time": "2024-12-07T05:39:58.365385",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.359425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Encoding Categorical Variables\n",
    "Categorical variables, such as cp, restecg, slope, ca, and thal, were transformed using one-hot encoding. This process was essential for converting categorical data into a numerical format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f03e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.379481Z",
     "iopub.status.busy": "2024-12-07T05:39:58.379050Z",
     "iopub.status.idle": "2024-12-07T05:39:58.398202Z",
     "shell.execute_reply": "2024-12-07T05:39:58.397262Z"
    },
    "id": "wVXM-GR2LOKN",
    "papermill": {
     "duration": 0.029249,
     "end_time": "2024-12-07T05:39:58.400791",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.371542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['cp', 'restecg', 'slope', 'ca', 'thal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ebf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.414539Z",
     "iopub.status.busy": "2024-12-07T05:39:58.414168Z",
     "iopub.status.idle": "2024-12-07T05:39:58.419962Z",
     "shell.execute_reply": "2024-12-07T05:39:58.418944Z"
    },
    "id": "Czeq73rBhH9q",
    "outputId": "7eadd5fc-9795-49dd-ae7d-e6457e2124db",
    "papermill": {
     "duration": 0.015438,
     "end_time": "2024-12-07T05:39:58.422311",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.406873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fda8bf",
   "metadata": {
    "id": "Bm1_xrY2LRtq",
    "papermill": {
     "duration": 0.006029,
     "end_time": "2024-12-07T05:39:58.434689",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.428660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Feature Scaling\n",
    "Numerical features, including age, trestbps, chol, thalach, and oldpeak, were standardized to ensure consistency in their scale. This step involved adjusting the features to have a mean of 0 and a standard deviation of 1, thereby enhancing the model's ability to learn effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138db98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:58.449696Z",
     "iopub.status.busy": "2024-12-07T05:39:58.449334Z",
     "iopub.status.idle": "2024-12-07T05:39:59.878869Z",
     "shell.execute_reply": "2024-12-07T05:39:59.877753Z"
    },
    "id": "0rMfQsLULS17",
    "papermill": {
     "duration": 1.440814,
     "end_time": "2024-12-07T05:39:59.881774",
     "exception": false,
     "start_time": "2024-12-07T05:39:58.440960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']] = scaler.fit_transform(data[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:59.897497Z",
     "iopub.status.busy": "2024-12-07T05:39:59.896907Z",
     "iopub.status.idle": "2024-12-07T05:39:59.922966Z",
     "shell.execute_reply": "2024-12-07T05:39:59.920912Z"
    },
    "id": "7lvIRUvJJsdq",
    "outputId": "baa786c9-6384-42d3-f0c4-00ed2449a8d2",
    "papermill": {
     "duration": 0.037617,
     "end_time": "2024-12-07T05:39:59.925934",
     "exception": false,
     "start_time": "2024-12-07T05:39:59.888317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858acff",
   "metadata": {
    "id": "BvK1APBfLWBB",
    "papermill": {
     "duration": 0.0066,
     "end_time": "2024-12-07T05:39:59.939671",
     "exception": false,
     "start_time": "2024-12-07T05:39:59.933071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 Data Splitting\n",
    "The preprocessed data was partitioned into training and testing sets using a stratified split. This method preserved the distribution of the target variable across the splits, ensuring that the model evaluation would be both reliable and representative of its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125c939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:39:59.955589Z",
     "iopub.status.busy": "2024-12-07T05:39:59.955190Z",
     "iopub.status.idle": "2024-12-07T05:40:00.099803Z",
     "shell.execute_reply": "2024-12-07T05:40:00.098339Z"
    },
    "id": "30wiG7QWLXX2",
    "papermill": {
     "duration": 0.155924,
     "end_time": "2024-12-07T05:40:00.102497",
     "exception": false,
     "start_time": "2024-12-07T05:39:59.946573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93dad24",
   "metadata": {
    "id": "bAC30bYVNkFm",
    "papermill": {
     "duration": 0.006555,
     "end_time": "2024-12-07T05:40:00.116295",
     "exception": false,
     "start_time": "2024-12-07T05:40:00.109740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Handling Class Imbalance\n",
    "To address the issue of class imbalance in the dataset, the BorderlineSMOTE technique was employed. BorderlineSMOTE generates synthetic samples for the minority class, particularly focusing on the instances that lie on the borderline between classes. This method enhances the learning process for the model by concentrating on samples near the decision boundary, which are more challenging to classify. By generating more synthetic samples for these critical instances, BorderlineSMOTE helps the model better distinguish between classes and reduces the risk of bias toward the majority class. This approach is essential for improving the model's overall predictive performance and ensuring a more balanced representation of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69561edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:40:00.131677Z",
     "iopub.status.busy": "2024-12-07T05:40:00.130774Z",
     "iopub.status.idle": "2024-12-07T05:40:01.161279Z",
     "shell.execute_reply": "2024-12-07T05:40:01.160225Z"
    },
    "id": "rH6AT36cNk6u",
    "outputId": "0cd73999-c75a-4a85-9f5d-6b4e9f6d191f",
    "papermill": {
     "duration": 1.041204,
     "end_time": "2024-12-07T05:40:01.163911",
     "exception": false,
     "start_time": "2024-12-07T05:40:00.122707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Initial class distribution\n",
    "before_counts = y_train.value_counts()\n",
    "before_labels = before_counts.index\n",
    "before_sizes = before_counts.values\n",
    "\n",
    "# Step 1: Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 2: Apply Borderline-SMOTE\n",
    "borderline_smote = BorderlineSMOTE(sampling_strategy='all', k_neighbors=15, random_state=42)\n",
    "X_borderline_smote, y_borderline_smote = borderline_smote.fit_resample(X_smote, y_smote)\n",
    "\n",
    "# Step 3: Apply Tomek Links\n",
    "tomek = TomekLinks(sampling_strategy='all')\n",
    "X_final, y_final = tomek.fit_resample(X_borderline_smote, y_borderline_smote)\n",
    "\n",
    "# Class distribution after hybrid approach\n",
    "after_counts = y_final.value_counts()\n",
    "after_labels = after_counts.index\n",
    "after_sizes = after_counts.values\n",
    "\n",
    "# Plot Pie Charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Pie chart before hybrid approach\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(before_sizes, labels=before_labels, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff'])\n",
    "plt.title('Class Distribution Before Hybrid Approach')\n",
    "\n",
    "# Pie chart after hybrid approach\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(after_sizes, labels=after_labels, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff'])\n",
    "plt.title('Class Distribution After Hybrid Approach')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4703977",
   "metadata": {
    "id": "EYnA5GjAP4ye",
    "papermill": {
     "duration": 0.007193,
     "end_time": "2024-12-07T05:40:01.178554",
     "exception": false,
     "start_time": "2024-12-07T05:40:01.171361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Model Building\n",
    "The stacked ensemble model was developed to enhance predictive performance by combining multiple base classifiers. A diverse set of six base models was chosen to capture different aspects of the data, improving overall accuracy and robustness. The predictions from these base models were used as input features for a meta-model, which synthesized their outputs to make the final prediction. This approach exploits the complementary strengths of various algorithms, resulting in a more effective ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc935ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:40:01.195664Z",
     "iopub.status.busy": "2024-12-07T05:40:01.195132Z",
     "iopub.status.idle": "2024-12-07T05:40:33.015073Z",
     "shell.execute_reply": "2024-12-07T05:40:33.013856Z"
    },
    "id": "wMwyxGGFPyLB",
    "outputId": "6fc923b6-b6f9-42e9-8cef-85eeb9a2818c",
    "papermill": {
     "duration": 31.838947,
     "end_time": "2024-12-07T05:40:33.024798",
     "exception": false,
     "start_time": "2024-12-07T05:40:01.185851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_final)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)),\n",
    "    ('svc', SVC(probability=True, kernel='rbf', C=1.0, gamma='scale', random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5, algorithm='auto', p=2)),  # p=2 for Euclidean distance\n",
    "    ('log', LogisticRegression(max_iter=2000, C=0.5, solver='lbfgs', random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=2, random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression(max_iter=2000, C=0.5, solver='lbfgs')\n",
    "\n",
    "# Create stacking classifier\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "stacked_model.fit(X_train_scaled, y_final)\n",
    "\n",
    "# Predict and evaluate the stacked model\n",
    "y_pred_stacked = stacked_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred_stacked)\n",
    "report = classification_report(y_test, y_pred_stacked)\n",
    "\n",
    "# Output results\n",
    "print(f\"Stacked Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Cross-validation scores for base models\n",
    "for name, model in base_models:\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_final, cv=5, scoring='accuracy')\n",
    "    print(f\"\\n{name} Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"{name} Mean CV Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Cross-validation scores for stacked model\n",
    "stacked_cv_scores = cross_val_score(stacked_model, X_train_scaled, y_final, cv=5, scoring='accuracy')\n",
    "print(f\"\\nStacked Model Cross-Validation Mean Accuracy: {stacked_cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab9ab2",
   "metadata": {
    "id": "VNDULs7CQIi8",
    "papermill": {
     "duration": 0.007531,
     "end_time": "2024-12-07T05:40:33.041793",
     "exception": false,
     "start_time": "2024-12-07T05:40:33.034262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Description of Base Models:\n",
    "\n",
    "* RandomForestClassifier: A versatile ensemble method using multiple decision trees to improve accuracy and reduce overfitting.\n",
    "* SVC (Support Vector Classifier): A classifier that finds the optimal hyperplane for separating classes, using probability estimates.\n",
    "* KNeighborsClassifier: A non-parametric method that classifies based on the majority vote of neighboring samples.\n",
    "* LogisticRegression: A linear model for binary classification, used as both a base model and meta-model for stacking.\n",
    "* ExtraTreesClassifier: An ensemble method that creates multiple decision trees and averages their predictions, similar to Random Forest but with additional randomness.\n",
    "\n",
    "* DecisionTreeClassifier: A simple classifier that splits the data based on feature values to form a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b2539",
   "metadata": {
    "id": "At5HguNqVXgZ",
    "papermill": {
     "duration": 0.007234,
     "end_time": "2024-12-07T05:40:33.056708",
     "exception": false,
     "start_time": "2024-12-07T05:40:33.049474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Model Accuracy Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4042f",
   "metadata": {
    "id": "JfOQhJV8Z10M",
    "papermill": {
     "duration": 0.009184,
     "end_time": "2024-12-07T05:40:33.073628",
     "exception": false,
     "start_time": "2024-12-07T05:40:33.064444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1 Accuracy Evaluation\n",
    "\n",
    "The accuracy of each model was evaluated and visualized using a bar chart. This chart highlights the performance of individual base models compared to the stacked ensemble model. The stacked model achieved an accuracy of 98.5%, demonstrating superior performance relative to the base models. The bar chart provides a clear comparison, showcasing how well each model classifies the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc3269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:40:33.097492Z",
     "iopub.status.busy": "2024-12-07T05:40:33.097141Z",
     "iopub.status.idle": "2024-12-07T05:40:34.318500Z",
     "shell.execute_reply": "2024-12-07T05:40:34.317101Z"
    },
    "id": "Ep09B7kDQhD3",
    "outputId": "fb290a69-00db-487e-a134-b68fa0ec3493",
    "papermill": {
     "duration": 1.234182,
     "end_time": "2024-12-07T05:40:34.320871",
     "exception": false,
     "start_time": "2024-12-07T05:40:33.086689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate base models\n",
    "base_model_accuracies = {}\n",
    "for name, model in base_models:\n",
    "    model.fit(X_final, y_final)\n",
    "    y_pred_base = model.predict(X_test_scaled)  # Use X_test_scaled for consistency\n",
    "    accuracy = accuracy_score(y_test, y_pred_base) * 100  # Convert to percentage\n",
    "    base_model_accuracies[name] = accuracy\n",
    "\n",
    "# Evaluate the stacked model\n",
    "y_pred_stacked = stacked_model.predict(X_test_scaled)  # Use X_test_scaled for consistency\n",
    "stacked_accuracy = accuracy_score(y_test, y_pred_stacked) * 100  # Convert to percentage\n",
    "base_model_accuracies['Stacked Model'] = stacked_accuracy\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure()\n",
    "names = list(base_model_accuracies.keys())\n",
    "scores = list(base_model_accuracies.values())\n",
    "bars = plt.barh(names, scores, color='skyblue')\n",
    "\n",
    "# Add percentage labels to bars\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{bar.get_width():.1f}%',\n",
    "             va='center', ha='left', color='black')\n",
    "\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b63b6",
   "metadata": {
    "id": "73-LzVGAZ-bd",
    "papermill": {
     "duration": 0.008345,
     "end_time": "2024-12-07T05:40:34.338236",
     "exception": false,
     "start_time": "2024-12-07T05:40:34.329891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2 Precision, Recall, and F1-Score Metrics\n",
    "\n",
    "The precision, recall, and F1-score for each model are summarized in a table. Precision reflects the proportion of true positive predictions among all positive predictions, while recall indicates the model's ability to identify actual positives. The F1-score combines precision and recall into a single metric. The table shows these metrics rounded to two decimal places, with the stacked model showing the highest scores across all metrics, underscoring its effectiveness in classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed87c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:40:34.356534Z",
     "iopub.status.busy": "2024-12-07T05:40:34.356116Z",
     "iopub.status.idle": "2024-12-07T05:40:35.637979Z",
     "shell.execute_reply": "2024-12-07T05:40:35.636793Z"
    },
    "id": "GAMabzIuQr9b",
    "outputId": "a3ddbd47-b58b-4992-b3ea-85033c31960c",
    "papermill": {
     "duration": 1.297283,
     "end_time": "2024-12-07T05:40:35.643945",
     "exception": false,
     "start_time": "2024-12-07T05:40:34.346662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-Score': []\n",
    "}\n",
    "\n",
    "# Evaluate base models\n",
    "for name, model in base_models:\n",
    "    model.fit(X_final, y_final)\n",
    "    y_pred_base = model.predict(X_test_scaled)  # Use X_test_scaled for consistency\n",
    "    precision = round(precision_score(y_test, y_pred_base) * 100, 2)  # Convert to percentage and round\n",
    "    recall = round(recall_score(y_test, y_pred_base) * 100, 2)  # Convert to percentage and round\n",
    "    f1 = round(f1_score(y_test, y_pred_base) * 100, 2)  # Convert to percentage and round\n",
    "    metrics['Model'].append(name)\n",
    "    metrics['Precision'].append(precision)\n",
    "    metrics['Recall'].append(recall)\n",
    "    metrics['F1-Score'].append(f1)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "y_pred_stacked = stacked_model.predict(X_test_scaled)  # Use X_test_scaled for consistency\n",
    "precision_stacked = round(precision_score(y_test, y_pred_stacked) * 100, 2)  # Convert to percentage and round\n",
    "recall_stacked = round(recall_score(y_test, y_pred_stacked) * 100, 2)  # Convert to percentage and round\n",
    "f1_stacked = round(f1_score(y_test, y_pred_stacked) * 100, 2)  # Convert to percentage and round\n",
    "metrics['Model'].append('Stacked Model')\n",
    "metrics['Precision'].append(precision_stacked)\n",
    "metrics['Recall'].append(recall_stacked)\n",
    "metrics['F1-Score'].append(f1_stacked)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot table\n",
    "fig, ax = plt.subplots(figsize=(9, 5))  # Adjust the size of the figure\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table_data = metrics_df.values\n",
    "table = ax.table(cellText=table_data, colLabels=metrics_df.columns, cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "\n",
    "# Adjust font size and scale\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width([0, 1, 2, 3])\n",
    "\n",
    "plt.title('Model Evaluation Metrics', fontsize=14, pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77bbc3",
   "metadata": {
    "id": "OGvsFtTEbG4_",
    "papermill": {
     "duration": 0.015566,
     "end_time": "2024-12-07T05:40:35.685556",
     "exception": false,
     "start_time": "2024-12-07T05:40:35.669990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Model Saving\n",
    "\n",
    "In this stage, the trained machine learning models are saved to disk to facilitate future use without requiring re-training. For this purpose, the Python pickle library is utilized. pickle is a standard library in Python that enables serialization and deserialization of Python objects. It converts the trained model into a byte stream, which can be stored as a file and later reconstructed into the original model.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "* Serialization: Using pickle, the trained model is serialized into a binary format and saved to a file. This allows the model to be stored efficiently and loaded later for predictions or further evaluation.\n",
    "\n",
    "* Deserialization: When needed, the model can be loaded back into memory from the saved file using pickle. This restores the model to its original state, enabling its use in generating predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e495a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T05:40:35.706805Z",
     "iopub.status.busy": "2024-12-07T05:40:35.706440Z",
     "iopub.status.idle": "2024-12-07T05:40:35.764338Z",
     "shell.execute_reply": "2024-12-07T05:40:35.763106Z"
    },
    "id": "mSkKPHDHXr4H",
    "papermill": {
     "duration": 0.071352,
     "end_time": "2024-12-07T05:40:35.767084",
     "exception": false,
     "start_time": "2024-12-07T05:40:35.695732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Save the model and scaler\n",
    "with open('heart.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'model': stacked_model,\n",
    "        'scaler': scaler\n",
    "    }, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ Load the model safely whether it's a dict or just the model\n",
    "with open('heart.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# ✅ If it's a dict (common case), extract the model\n",
    "if isinstance(loaded_data, dict):\n",
    "    final_model = loaded_data.get('model')\n",
    "    if final_model is None:\n",
    "        raise ValueError(\"❌ The 'model' key was not found in the loaded dictionary.\")\n",
    "else:\n",
    "    final_model = loaded_data  # It's already the model\n",
    "\n",
    "# ✅ Make sure X_test is defined and preprocessed correctly\n",
    "# Example: Load from CSV if you have test data saved\n",
    "# X_test = pd.read_csv('test_data.csv')  # Uncomment and modify as needed\n",
    "\n",
    "# ❗ Check if X_test is available\n",
    "try:\n",
    "    X_test\n",
    "except NameError:\n",
    "    raise ValueError(\"❌ 'X_test' is not defined. Please define or load it before making predictions.\")\n",
    "try:\n",
    "    print(model.feature_names_in_)\n",
    "except AttributeError:\n",
    "    print(\"❌ The model does not store feature names. You need to provide the original feature order manually.\")\n",
    "\n",
    "# ✅ Predict probabilities and classes\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "percentage_predictions = y_proba * 100\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# ✅ Create recommendations\n",
    "recommendations = []\n",
    "for i, (pct, pred) in enumerate(zip(percentage_predictions, y_pred)):\n",
    "    print(f\"Patient {i+1}: 💓 Heart Disease Risk = {pct:.2f}%\")\n",
    "    if pct >= 80:\n",
    "        msg = \"HIGH RISK - Seek immediate medical consultation\"\n",
    "        print(\"🛑 Recommendation:\", msg + \"\\n\")\n",
    "    elif pct >= 50:\n",
    "        msg = \"MODERATE RISK - Schedule a heart health check-up soon\"\n",
    "        print(\"⚠️ Recommendation:\", msg + \"\\n\")\n",
    "    else:\n",
    "        msg = \"LOW RISK - Maintain a healthy lifestyle\"\n",
    "        print(\"✅ Recommendation:\", msg + \"\\n\")\n",
    "    recommendations.append((msg, pct, pred))\n",
    "\n",
    "# ✅ Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Patient': [i + 1 for i in range(len(y_pred))],\n",
    "    'Heart Disease Risk (%)': percentage_predictions,\n",
    "    'Prediction (Binary)': y_pred,\n",
    "    'Recommendation': [rec[0] for rec in recommendations],\n",
    "    'Risk Percentage': [rec[1] for rec in recommendations]\n",
    "})\n",
    "results_df.to_csv('heart_disease_predictions_with_recommendations.csv', index=False)\n",
    "print(\"\\n✅ All predictions and recommendations saved to CSV.\")\n",
    "\n",
    "# ✅ Save model + predictions to one .pkl file\n",
    "with open('prediction.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': final_model,\n",
    "        'X_test': X_test,\n",
    "        'y_proba': y_proba,\n",
    "        'percentage_predictions': percentage_predictions,\n",
    "        'y_pred': y_pred,\n",
    "        'recommendations': recommendations,\n",
    "        'results_df': results_df\n",
    "    }, f)\n",
    "\n",
    "print(\"📦 Final model and predictions saved to 'final_model_with_predictions.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 216167,
     "sourceId": 477177,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.099717,
   "end_time": "2024-12-07T05:40:36.499116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-07T05:39:54.399399",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
