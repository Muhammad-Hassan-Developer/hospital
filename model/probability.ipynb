{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c970a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9609756097560975\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       102\n",
      "           1       0.98      0.94      0.96       103\n",
      "\n",
      "    accuracy                           0.96       205\n",
      "   macro avg       0.96      0.96      0.96       205\n",
      "weighted avg       0.96      0.96      0.96       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "\n",
    "# === Step 1: Load and preprocess raw data ===\n",
    "data = pd.read_csv(\"heart.csv\")  # Replace with actual path\n",
    "\n",
    "original_features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "                     'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "# Separate features and target\n",
    "X = data[original_features]\n",
    "y = data[target]\n",
    "\n",
    "# Save original column names\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
    "\n",
    "# === Step 2: Create Preprocessing Pipeline ===\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "], remainder='passthrough')  # 'sex', 'fbs', 'exang' pass through\n",
    "\n",
    "# Apply preprocessing\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# === Step 3: Split and Resample ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 2: Borderline SMOTE\n",
    "borderline_smote = BorderlineSMOTE(sampling_strategy='all', k_neighbors=15, random_state=42)\n",
    "X_borderline, y_borderline = borderline_smote.fit_resample(X_smote, y_smote)\n",
    "\n",
    "# Step 3: Tomek Links\n",
    "tomek = TomekLinks(sampling_strategy='all')\n",
    "X_final, y_final = tomek.fit_resample(X_borderline, y_borderline)\n",
    "\n",
    "# === Step 4: Define Models ===\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)),\n",
    "    ('svc', SVC(probability=True, kernel='rbf', C=1.0, gamma='scale', random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5, algorithm='auto', p=2)),\n",
    "    ('log', LogisticRegression(max_iter=2000, C=0.5, solver='lbfgs', random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=2, random_state=42))\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=2000, C=0.5, solver='lbfgs')\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# === Step 5: Train the Model ===\n",
    "stacked_model.fit(X_final, y_final)\n",
    "\n",
    "# === Step 6: Evaluate ===\n",
    "X_test_scaled = X_test  # Already scaled by preprocessor\n",
    "y_pred = stacked_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# === Step 7: Wrap everything in a class ===\n",
    "class HeartDiseasePredictor:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.feature_order = original_features\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        input_data: pd.DataFrame with original 13 columns (raw, unencoded)\n",
    "        Returns prediction and probability\n",
    "        \"\"\"\n",
    "        if not all(col in input_data.columns for col in self.feature_order):\n",
    "            raise ValueError(f\"Missing columns in input data. Required: {self.feature_order}\")\n",
    "        transformed = self.preprocessor.transform(input_data[self.feature_order])\n",
    "        prediction = self.model.predict(transformed)\n",
    "        probability = self.model.predict_proba(transformed)[:, 1] * 100  # percentage\n",
    "        return prediction, probability\n",
    "\n",
    "# Create final object\n",
    "final_model = HeartDiseasePredictor(model=stacked_model, preprocessor=preprocessor)\n",
    "\n",
    "# === Step 8: Save Final Model ===\n",
    "with open(\"final_heart.pkl\", \"wb\") as file:\n",
    "    pickle.dump(final_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "209b51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class HeartDiseasePredictor:\n",
    "    def __init__(self, model, scaler, dummy_columns):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.dummy_columns = dummy_columns  # List of final column names after get_dummies()\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        # Step 1: One-hot encode same as training\n",
    "        df_encoded = pd.get_dummies(df, columns=['cp', 'restecg', 'slope', 'ca', 'thal'])\n",
    "\n",
    "        # Step 2: Align columns with training data\n",
    "        for col in self.dummy_columns:\n",
    "            if col not in df_encoded.columns:\n",
    "                df_encoded[col] = 0\n",
    "        df_encoded = df_encoded[self.dummy_columns]\n",
    "\n",
    "        # Step 3: Scale numeric features\n",
    "        cols_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "        df_encoded[cols_to_scale] = self.scaler.transform(df_encoded[cols_to_scale])\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        processed = self.preprocess(input_df)\n",
    "        prediction = self.model.predict(processed)\n",
    "        probability = self.model.predict_proba(processed)[0][1] * 100\n",
    "        return prediction, round(probability, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ddbc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = HeartDiseasePredictor(model=stacked_model, scaler=scaler, dummy_columns=X.columns.tolist())\n",
    "\n",
    "import pickle\n",
    "with open('final_heart.pkl', 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "285eee32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 13 features, but RandomForestClassifier is expecting 22 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Raw input with 13 features\u001b[39;00m\n\u001b[32m      9\u001b[39m new_data = pd.DataFrame([{\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m21\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msex\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mthal\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m\n\u001b[32m     23\u001b[39m }])\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m prediction, probability = \u001b[43mmodel_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, prediction[\u001b[32m0\u001b[39m])\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProbability of Heart Disease: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobability\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mHeartDiseasePredictor.predict\u001b[39m\u001b[34m(self, input_df)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_df):\n\u001b[32m     26\u001b[39m     processed = \u001b[38;5;28mself\u001b[39m.preprocess(input_df)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     prediction = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     probability = \u001b[38;5;28mself\u001b[39m.model.predict_proba(processed)[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m] * \u001b[32m100\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, \u001b[38;5;28mround\u001b[39m(probability, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:761\u001b[39m, in \u001b[36mStackingClassifier.predict\u001b[39m\u001b[34m(self, X, **predict_params)\u001b[39m\n\u001b[32m    758\u001b[39m     routed_params.final_estimator_ = Bunch(predict={})\n\u001b[32m    759\u001b[39m     routed_params.final_estimator_.predict = predict_params\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m y_pred = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinal_estimator_\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._label_encoder, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    763\u001b[39m     \u001b[38;5;66;03m# Handle the multilabel-indicator case\u001b[39;00m\n\u001b[32m    764\u001b[39m     y_pred = np.array(\n\u001b[32m    765\u001b[39m         [\n\u001b[32m    766\u001b[39m             \u001b[38;5;28mself\u001b[39m._label_encoder[target_idx].inverse_transform(target)\n\u001b[32m    767\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m target_idx, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_pred.T)\n\u001b[32m    768\u001b[39m         ]\n\u001b[32m    769\u001b[39m     ).T\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:374\u001b[39m, in \u001b[36m_BaseStacking.predict\u001b[39m\u001b[34m(self, X, **predict_params)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[32m    354\u001b[39m \n\u001b[32m    355\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m \u001b[33;03m    Predicted targets.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    373\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.final_estimator_.predict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, **predict_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:840\u001b[39m, in \u001b[36mStackingClassifier.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    826\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return class labels or probabilities for X for each estimator.\u001b[39;00m\n\u001b[32m    827\u001b[39m \n\u001b[32m    828\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    838\u001b[39m \u001b[33;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[32m    839\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:297\u001b[39m, in \u001b[36m_BaseStacking._transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    296\u001b[39m predictions = [\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_, \u001b[38;5;28mself\u001b[39m.stack_method_)\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m est != \u001b[33m\"\u001b[39m\u001b[33mdrop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m ]\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._concatenate_predictions(X, predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:946\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    944\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    949\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:638\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hassa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2830\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2831\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2832\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 13 features, but RandomForestClassifier is expecting 22 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "with open('final_heart.pkl', 'rb') as file:\n",
    "    model_obj = pickle.load(file)\n",
    "\n",
    "# Raw input with 13 features\n",
    "new_data = pd.DataFrame([{\n",
    "    'age': 21,\n",
    "    'sex': 1,\n",
    "    'cp': 0,\n",
    "    'trestbps': 115,\n",
    "    'chol': 170,\n",
    "    'fbs': 0,\n",
    "    'restecg': 1,\n",
    "    'thalach': 180,\n",
    "    'exang': 0,\n",
    "    'oldpeak': 0.0,\n",
    "    'slope': 2,\n",
    "    'ca': 0,\n",
    "    'thal': 2\n",
    "}])\n",
    "\n",
    "prediction, probability = model_obj.predict(new_data)\n",
    "\n",
    "print(\"Prediction:\", prediction[0])\n",
    "print(f\"Probability of Heart Disease: {probability}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
